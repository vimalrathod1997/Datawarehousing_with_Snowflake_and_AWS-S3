
--Create a data warehouse for storing your data 's3_warehouse_data' from aws s3 bucket
--You can create a warehouse through web interface too

CREATE WAREHOUSE DWH_Netflix_Warehouse WITH 
WAREHOUSE_SIZE = 'XSMALL' 
WAREHOUSE_TYPE = 'STANDARD' 
AUTO_SUSPEND = 600 
AUTO_RESUME = TRUE 
COMMENT = 'This data warehouse is created for the netflix data staged in AWS s3 bucket';



--Create a database in your warehouse

CREATE DATABASE DB_Netflix_Database;

USE DB_Netflix_Database;



--Create a table in your database to store the data which would be extracted from s3 bucket

CREATE TABLE tbl_Netflix_trail (
dwh_movie_id varchar,	
dwh_movie_or_show varchar,	
dwh_year_of_release	varchar,
dwh_age_certification_pg_rating	varchar,
dwh_runtime_in_minutes varchar,	
dwh_genres varchar,	
dwh_production_countries varchar,	
dwh_number_of_seasons int,	
dwh_imdb_score double PRECISION,	
dwh_imdb_votes int,	
dwh_tmdb_popularity	double PRECISION,
dwh_tmdb_score double PRECISION,	
dwh_name_of_the_person varchar,	
dwh_role_of_the_person varchar
);



--Create a Storage Integration, that would enable us to extract data from AWS S3 bucket without providing aws credentials/Access and Secret keys.
--This method ensures better privacty and security, because the critical and sensitive credentials and Access/Secret keys aren't exposed.

CREATE OR REPLACE STORAGE INTEGRATION S3_Netflix_Integration
 TYPE = EXTERNAL_STAGE
 STORAGE_PROVIDER = S3
 STORAGE_AWS_ROLE_ARN = 'enter the aws arn of a role created in AWS IAM. This role would have permissions and policies to interact/access with s3 bucket'
 ENABLED = TRUE
 STORAGE_ALLOWED_LOCATIONS = ('s3://bucket-name/folder-name/');
 
 
 
 --get the 'aws iam arn' provided by snowflake by executing the below query. The below query returns the description of the storage integration
 
 DESC integration S3_Netflix_Integration;
 
 
 
--edit the trust relationship of our iam-role and update principal>AWS and sts.ExternalID with STORAGE_AWS_IAM_USER_ARN
--and STORAGE_AWS_EXTERNAL_ID provided by snowflake. 
--Create a File format. This would help Snowflake to interpret the file we'll ingest from our s3 bucket.
 
 CREATE OR REPLACE FILE FORMAT CSV_Format
 TYPE = CSV
 FIELD_DELIMITER = ','
 ERROR_ON_COLUMN_COUNT_MISMATCH=False
 SKIP_HEADER = 1; 
 
 
 
--Create a stage for our data. In snowflake all kinds of data extraction and ingestion requires a staging area.
--Our target data needs to be uploaded into a staging area and then snowflake would extract the data from the respective staging area to load into the virtual warehouse 
--Staging area can be internal or external

CREATE OR REPLACE STAGE stg_Netflix_Stage
URL = 's3://snowflake-warehouse-staging-bucket/staging_Area/'
STORAGE_INTEGRATION = S3_Netflix_Integration
FILE_FORMAT = CSV_Format;

Show Stages;



--Confirm if the data is staged properly. 
--The below query would return information about the stage. If it returns data and 'size' column  has some data in it that means our data has staged properly.

list @stg_Netflix_Stage;




--Copy data from staging area into netflix table.

Copy into tbl_Netflix_trail from @stg_Netflix_Stage_Data Pattern ='.*.csv' ON_Error=Continue



-- Create a snowpipe for auto ingestion of data from our staging area.

Create or replace pipe netflix_pipe auto_ingest=true as 
Copy into tbl_netflix from @stg_Netflix_Stage_Data Pattern ='.*.csv' ON_Error=Continue



--Execute the below query to get description of our snowpipe. The query would return the value of 'notification channel' that contains the sqs 
arn

show pipes;



 